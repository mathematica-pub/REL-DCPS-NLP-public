{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c045932-85be-4827-83f1-fd7df0e15ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "import os\n",
    "import time\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms import LlamaCpp \n",
    "\n",
    "\n",
    "## Define text feedback function\n",
    "def generate_text(input_prompts : List[str],\n",
    "                 system_prompt: str = \"You are a helpful assistant.\", \n",
    "                 model_path: str = \"./model/Meta-Llama-3-8B-Instruct-Q3_K_S.gguf\" ,\n",
    "                 temperature: float = 0.3, \n",
    "                 n_ctx: int = 512, \n",
    "                 max_tokens: int = 256) -> dict: \n",
    "    \"\"\"\n",
    "    Generate text using a LlamaCpp model. \n",
    "\n",
    "    Args: \n",
    "        input_prompts (List[str]): List of prompts to generate text for.\n",
    "        system_prompt (str): System prompt for the model.\n",
    "        model_path (str): Path to the model file.\n",
    "        temperature (float): Sampling temperature for model output.\n",
    "        n_ctx (int): Number of tokens in the context window.\n",
    "        max_tokens (int): Maximum tokens to generate in a single call.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Ensure model path exists\n",
    "    assert os.path.exists(model_path)\n",
    "    \n",
    "    ## set batch size\n",
    "    n_batch = n_ctx // 4\n",
    "                   \n",
    "    ## Instruct\n",
    "    # Load the LlamaCpp language model, adjust GPU usage based on your hardware\n",
    "    llm = LlamaCpp(\n",
    "        # Path to model\n",
    "        model_path=model_path,\n",
    "        # Number of tokens in the context window\n",
    "        n_ctx=n_ctx, \n",
    "        # Batch size for model processing\n",
    "        n_batch=n_batch,   \n",
    "        # Maximum tokens to generate in a single call\n",
    "        max_tokens=max_tokens, \n",
    "        # Sampling temperature for model output \n",
    "        temperature=temperature, \n",
    "        # Tokens to indicate when to stop generating\n",
    "        stop=[\"<|eot_id|>\",  \"assistant\\n\\n\"] \n",
    "        )\n",
    "\n",
    "    # Define the prompt template with a placeholder for the question\n",
    "    prompt_end = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{user_input}\" + prompt_end),\n",
    "        ])   \n",
    "\n",
    "    # Create an LLMChain to manage interactions with the prompt and model\n",
    "    chain = template | llm \n",
    "\n",
    "    # Invoke the chain\n",
    "    batch_prompts = [{\"user_input\": p} for p in input_prompts]\n",
    "\n",
    "    start = time.time()\n",
    "    responses = chain.batch(batch_prompts)\n",
    "    end = time.time()\n",
    "    \n",
    "    elapsed = end - start\n",
    "\n",
    "    # Return output\n",
    "    output = {\"system_prompt\": system_prompt,\n",
    "              \"prompts\": input_prompts, \n",
    "              \"responses\": responses, \n",
    "              \"elapsed\": elapsed}\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2546899a-712d-4971-aa69-a167e18534f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Generate responses for basic prompts\n",
    "model1 = \"C:/Users/pkress/projects/REL-DCPS-workshop/REL-DCPS-NLP-public/model/Meta-Llama-3-8B-Instruct-Q3_K_S.gguf\" \n",
    "basic_prompts = [\"Tell me a joke\", \"Explain the big bang in simple terms\"]\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "basic_small = generate_text(basic_prompts,\n",
    "                 system_prompt = system_prompt, \n",
    "                 model_path = model1,\n",
    "                 temperature = 0.3, \n",
    "                 n_ctx = 128, max_tokens = 256 )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bf93c99-56cf-4c14-95d4-a6541b423f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\pkress\\\\projects\\\\REL-DCPS-workshop\\\\REL-DCPS-NLP-public'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbdaef8a-f040-4966-a91c-dee2e90b4257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(\"C:/Users/pkress/projects/REL-DCPS-workshop/REL-DCPS-NLP-public/model//Meta-Llama-3-8B-Instruct-Q3_K_S.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d93dcf8e-4176-4de4-86e4-0e4cddbb0778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system_prompt': 'You are a helpful assistant.', 'prompts': ['Tell me a joke', 'Explain the big bang in simple terms'], 'responses': [\"\\n\\nHere's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you smile!\", '\\n\\nHello there!\\n\\nSo, you want to know about the Big Bang? Well, let me tell you all about it!\\n\\nThe Big Bang was the massive explosion that marked the beginning of our universe.\\n\\nAbout 13.8 billion years ago, a single point called the singularity existed. This singularity contained all the matter and energy that would eventually make up our universe.\\n\\nThen, suddenly and explosively, this singularity expanded rapidly, releasing an enormous amount of energy in the process.\\n\\nThis explosion marked the beginning of our'], 'elapsed': 31.375819206237793}\n",
      "\n",
      "\n",
      "Here's one:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "(Wait for it...)\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you smile!\n",
      "\n",
      "\n",
      "Hello there!\n",
      "\n",
      "So, you want to know about the Big Bang? Well, let me tell you all about it!\n",
      "\n",
      "The Big Bang was the massive explosion that marked the beginning of our universe.\n",
      "\n",
      "About 13.8 billion years ago, a single point called the singularity existed. This singularity contained all the matter and energy that would eventually make up our universe.\n",
      "\n",
      "Then, suddenly and explosively, this singularity expanded rapidly, releasing an enormous amount of energy in the process.\n",
      "\n",
      "This explosion marked the beginning of our\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(basic_small)\n",
    "[print(p) for p in basic_small[\"responses\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52121ec-7cb8-4f7b-a7de-1216f3f28600",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Now read in the generate text function from the primary script\n",
    "from primary.generate_text import generate_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
