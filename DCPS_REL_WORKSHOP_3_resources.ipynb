{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4db54d3",
   "metadata": {},
   "source": [
    "# Structure of this notebook\n",
    "\n",
    "- Discussion of potential important tools\n",
    "    - Post-processing\n",
    "    - OpenAI on Azure\n",
    "- Discussion of additional more advanced LLM topics\n",
    "    - Prompt Engineering\n",
    "    - RAG (Retrieval Augmented Generation)\n",
    "    - Fine-tuning (pros and cons)\n",
    "    - Agents?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faafdd61",
   "metadata": {},
   "source": [
    "# Potential key tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d2df07",
   "metadata": {},
   "source": [
    "## Post-Processing\n",
    "\n",
    "We now have long text output based on our prompts and goals. However, to make use of this text, we need to convert it into a usable format. \n",
    "\n",
    "A classic approach is to extract the numbers for each category using regular expressions. Since the OpenAI output was most consistant in it's format, we can write a quick function to extract the scores in python. You could easily do a similar extraction/cleaning step in R or stata if those are preferred for analysis. \n",
    "\n",
    "Our example is in `snippet_postprocess_responses.py`. Additionally, [this](https://learnbyexample.github.io/python-regex-cheatsheet/) helpful guide gives a comprehensive description of regular expressions in python and many examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92fe918",
   "metadata": {},
   "source": [
    "## OpenAI on Azure\n",
    "\n",
    "OpenAI has a partnership with Azure such that OpenAI models can be run securely, without transferring any information to OpenAI. \n",
    "\n",
    "This guide walks you through the process (focus on the OpenAI Python 1.X approach)\n",
    "\n",
    "This requires filling out the application [here](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUNTZBNzRKNlVQSFhZMU9aV09EVzYxWFdORCQlQCN0PWcu). \n",
    "\n",
    "Then one needs to set up an OpenAI resource in Azure, most likely a Gpt3.5Turbo resource. From there, using the secure OpenAI models in Azure closely resembles using the general OpenAI API. \n",
    "\n",
    "Many of the key resources available in the general OpenAI API can be used securely in Azure, but not all of them. The Azure documentation helps clarify exactly what services are available.  \n",
    "\n",
    "The OpenAI example from the examples we shared will help introduce you to this approach. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a347244",
   "metadata": {},
   "source": [
    "# Additional more advanced LLM topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1db0e4",
   "metadata": {},
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "While fine-tuning can help with certain tasks, there are some risks associated with it and if possible it would be better to use prompt engineering or RAG (to be discussed later) as these will scale better if you switch to a different model provider or want to slightly alter the form of the desired output. \n",
    "\n",
    "A wide variety of prompt engineering guides exist. [Here](https://platform.openai.com/docs/guides/prompt-engineering/six-strategies-for-getting-better-results) is OpenAI's guide, [here](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview#prompting-vs-finetuning) is anthropic's, and [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/prompt-engineering) is Microsoft's. Additionally, [this paper](https://arxiv.org/html/2402.05201v1#S2) describes 5 common prompt engineering approaches. \n",
    "\n",
    "Even more complex prompt engineering approaches could allow us to embed context directly from the dataset into our prompts. To do this, we could edit our generate_text function to include another input beyond just `{user_input}`. \n",
    "\n",
    "An example might be to add `{grade_level}` to the prompt, so that the prompt template \"human\" section would read \n",
    "\n",
    "``` \n",
    "(\"human\", \"Grade: \" + {grade_level} + \". Goal: \" + {user_input}\" + prompt_end)\n",
    "``` \n",
    "\n",
    "\n",
    "Additionally, it is sometimes helpful to use LLMs to generate prompts. I made a very simple example of such a prompt-engineering conversation [here](https://chatgpt.com/share/9fb5f7d1-2b48-4ed0-8ad9-18b987f1fb43) in line with this example. Anthropic has a (paid) service specifically designed to help with prompt engineering, described [here](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f591d",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773d673",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation (RAG) takes this idea further, and uses the input prompt to query against a database of additional information and then embeds the most relevant queried information into the prompt. \n",
    "\n",
    "It can also encourage the response to rely on the database for answers. \n",
    "\n",
    "[This](https://towardsdatascience.com/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2) blog post gives a good brief overview. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163cc6ee",
   "metadata": {},
   "source": [
    "## Fine Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340024ad",
   "metadata": {},
   "source": [
    "Fine tuning is an additional option for streamlining output. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
